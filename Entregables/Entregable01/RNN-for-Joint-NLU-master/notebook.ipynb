{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "notebook.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXH-dFxty8Hc"
      },
      "source": [
        "# Ejecución de modelo base\n",
        "\n",
        "En este notebook se ha seleccionado como base el trabajo presentado por Bing Liu y Ian Lane denominado Attention-Based Recurrent Neural Network Models for Joint Intent Detection\n",
        "and Slot Filling: https://arxiv.org/pdf/1609.01454.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aeun0YXizAJX"
      },
      "source": [
        "## 1. Librerías a utilizar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzeTuLVNV0Nv"
      },
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import pickle\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "kmLtfYvSV0N0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06284cea-43cd-47f3-ea55-2a8c0577ad20"
      },
      "source": [
        "# Se verifica si es que el soporte de CUDA está disponible (para utilizar la GPU\n",
        "# en el cómputo de tensores)\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "USE_CUDA"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "EW6nzfcsV0N1"
      },
      "source": [
        "# Se define función para convertir la sequencia de texto a índices en formato de\n",
        "# tensor con soporte de CUDA\n",
        "def prepare_sequence(seq, to_ix):\n",
        "  # Crea una lista a partir de una secuencia. Para cada palabra de la secuencia \n",
        "  # asigna un índice de acuerdo al diccionario \"to_ix\", ya sea que pertenece o \n",
        "  # no (token desconocido <UNK>)\n",
        "  idxs = list(map(lambda w: to_ix[w] if w in to_ix.keys() else to_ix[\"<UNK>\"], seq))\n",
        "  # Convierte la lista en tensor con soporte de CUDA (si está disponible)\n",
        "  tensor = Variable(torch.LongTensor(idxs)).cuda() if USE_CUDA else Variable(torch.LongTensor(idxs))\n",
        "  return tensor\n",
        "\n",
        "# Se define función para obtener los elementos dentro de las listas de una tupla\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7eJEYcbV0N2"
      },
      "source": [
        "## 2. Carga de los datos y preprocesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwZyaaxQzj6a"
      },
      "source": [
        "El conjunto de entrenamiento se obtiene a partir del siguiente repositorio: https://github.com/yvchen/JointSLU/tree/master/data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idnTabKO1JIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98fff1d0-ada8-4baf-cd62-d3228f40231c"
      },
      "source": [
        "# Obtenemos únicamente el archivo que se utilizará\n",
        "!curl --remote-name -H --location https://raw.githubusercontent.com/yvchen/JointSLU/master/data/atis-2.train.w-intent.iob"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  789k  100  789k    0     0  3476k      0 --:--:-- --:--:-- --:--:-- 3476k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gOg6ptxIV0N2"
      },
      "source": [
        "# Cuando se ejecuta en un entorno local indicar correctamente el path general \n",
        "# del proyecto:\n",
        "# path = r'G:\\Mi unidad\\CLASES\\PUCP\\2.Clases\\Ciclo-II-DL\\Proyecto-DL\\JoinIntentDetectionSlotFilling\\Entregables\\Entregable01\\RNN-for-Joint-NLU-master'\n",
        "# train = open(path+\"\\\\\"+\"\\data\\\\atis-2.train.w-intent.iob\",\"r\").readlines()\n",
        "\n",
        "# Se lee la información del archivo con el corpus de ATIS\n",
        "train = open('atis-2.train.w-intent.iob', 'r').readlines()\n",
        "# Se eliminan los saltos de línea al final de la oración\n",
        "train = [t[:-1] for t in train]\n",
        "# Se obtiene la secuencia de entrada (oraciones en inglés), la secuencia de \n",
        "# salida (secuencia de slots) y la intención relacionada a la secuencia\n",
        "train = [[t.split('\\t')[0].split(' '), t.split('\\t')[1].split(' ')[:-1], t.split('\\t')[1].split(' ')[-1]] for t in train]\n",
        "# Se retiran los tokens de inicio y fin en la secuencia de entrada, el token de \n",
        "# inicio en la secuencia de salida (ya que se encuentra desfasada una posición \n",
        "# respecto a la entrada)\n",
        "train = [[t[0][1:-1],t[1][1:],t[2]] for t in train]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "rqx2iX5By0zz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76eb6208-8a40-4548-a80c-f81442e36bef"
      },
      "source": [
        "train[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['i',\n",
              "  'want',\n",
              "  'to',\n",
              "  'fly',\n",
              "  'from',\n",
              "  'baltimore',\n",
              "  'to',\n",
              "  'dallas',\n",
              "  'round',\n",
              "  'trip'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-fromloc.city_name',\n",
              "  'O',\n",
              "  'B-toloc.city_name',\n",
              "  'B-round_trip',\n",
              "  'I-round_trip'],\n",
              " 'atis_flight']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "JrJuooGFV0N3"
      },
      "source": [
        "# Se obtiene las tuplas de las secuencias de entrada, salida y las intenciones \n",
        "# por separado\n",
        "seq_in, seq_out, intent = list(zip(*train))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vbbrItN3V0N3"
      },
      "source": [
        "# Se conforma el vocabulario de las palabras, las etiquetas de los slots y las \n",
        "# etiquetas de las intenciones\n",
        "vocab = set(flatten(seq_in))\n",
        "slot_tag = set(flatten(seq_out))\n",
        "intent_tag = set(intent)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "o4BhYgIcV0N4"
      },
      "source": [
        "# Se establece el tamaño máximo de la secuencia\n",
        "LENGTH = 50"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WNJfftp6V0N4"
      },
      "source": [
        "# Se definen listas para almacenar las secuencias de entrada y salida luego del\n",
        "# preprocesamiento para añadir tokens de fin de secuencia y para hacer padding\n",
        "sin = []\n",
        "sout = []"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "NNn_dJ5bV0N4"
      },
      "source": [
        "# Se añade nuevamente el token de fin de secuencia (<EOS>) y se hace padding\n",
        "# hasta completar el tamaño máximo de secuencia\n",
        "for i in range(len(seq_in)):\n",
        "  # Secuencia de entrada\n",
        "  temp = seq_in[i]\n",
        "  if len(temp)<LENGTH:\n",
        "    # Añade el token de fin de secuencia\n",
        "    temp.append('<EOS>')\n",
        "    while len(temp)<LENGTH:\n",
        "      # Completa con token de padding hasta completar tamaño máximo de secuencia\n",
        "      temp.append('<PAD>')\n",
        "  else:\n",
        "    # Trunca la secuencia en el tamaño máximo\n",
        "    temp = temp[:LENGTH]\n",
        "    # Reemplaza el último elemento por el token de fin de secuencia\n",
        "    temp[-1]='<EOS>'\n",
        "  sin.append(temp)\n",
        "  \n",
        "  # Secuencia de salida\n",
        "  temp = seq_out[i]\n",
        "  if len(temp)<LENGTH:\n",
        "    while len(temp)<LENGTH:\n",
        "      # Completa con token de padding hasta completar tamaño máximo de secuencia\n",
        "      temp.append('<PAD>')\n",
        "  else:\n",
        "    # Trunca la secuencia en el tamaño máximo\n",
        "    temp = temp[:LENGTH]\n",
        "    # Reemplaza el último elemento por el token de fin de secuencia\n",
        "    temp[-1]='<EOS>'\n",
        "  sout.append(temp)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "UcaKGEd6V0N5"
      },
      "source": [
        "# Se define un diccionario para mapear las palabras a índices\n",
        "word2index = {'<PAD>': 0, '<UNK>':1, '<SOS>':2, '<EOS>':3}\n",
        "for token in vocab:\n",
        "  if token not in word2index.keys():\n",
        "    word2index[token]=len(word2index)\n",
        "\n",
        "# Se invierte el diccionario (para mapear los índices a palabras)\n",
        "index2word = {v:k for k,v in word2index.items()}\n",
        "\n",
        "# Se define un diccionario para mapear las etiquetas de slots a índices\n",
        "tag2index = {'<PAD>' : 0}\n",
        "for tag in slot_tag:\n",
        "  if tag not in tag2index.keys():\n",
        "    tag2index[tag] = len(tag2index)\n",
        "\n",
        "# Se invierte el diccionario (para mapear los índices a etiquetas de palabras)\n",
        "index2tag = {v:k for k,v in tag2index.items()}\n",
        "\n",
        "# Se define un diccionario para mapear las etiquetas de intenciones a índices\n",
        "intent2index={}\n",
        "for ii in intent_tag:\n",
        "  if ii not in intent2index.keys():\n",
        "    intent2index[ii] = len(intent2index)\n",
        "\n",
        "# Se invierte el diccionario (para mapear los índices a etiquetas de intenciones)\n",
        "index2intent = {v:k for k,v in intent2index.items()}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "uxvkvDXTV0N5"
      },
      "source": [
        "# Se reconstruye una lista general con las listas de secuencias de entrada, \n",
        "# salida y las intenciones\n",
        "train = list(zip(sin, sout, intent))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLA8jfqMV0N5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "caa0a734-cc29-42b6-cc82-638b1507fcd2"
      },
      "source": [
        "train[0][2]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'atis_flight'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aehAHe-qV0N6"
      },
      "source": [
        "# Se define una lista vacía para almacenar los tensores con soporte de CUDA \n",
        "# correspondientes a las secuencias de entrada, salida y las intenciones\n",
        "train_data=[]\n",
        "\n",
        "for tr in train:\n",
        "  # Se usa la función \"prepare_sequence\" para obtener el tensor de entrada con\n",
        "  # soporte de CUDA\n",
        "  temp = prepare_sequence(tr[0], word2index)\n",
        "  temp = temp.view(1,-1)\n",
        "  # Se usa la función \"prepare_sequence\" para obtener el tensor de salida con\n",
        "  # soporte de CUDA\n",
        "  temp2 = prepare_sequence(tr[1], tag2index)\n",
        "  temp2 = temp2.view(1,-1)\n",
        "  # Se usa la función \"prepare_sequence\" para obtener el tensor de intención \n",
        "  # con soporte de CUDA\n",
        "  temp3 = Variable(torch.LongTensor([intent2index[tr[2]]])).cuda() if USE_CUDA else Variable(torch.LongTensor([intent2index[tr[2]]]))\n",
        "  # Almacena los tensores en una lista\n",
        "  train_data.append((temp, temp2, temp3))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "C_A1ymkFV0N6"
      },
      "source": [
        "# Se define una función para producir batches aleatorios del conjunto de \n",
        "# entrenamiento \n",
        "def getBatch(batch_size, train_data):\n",
        "  random.shuffle(train_data)\n",
        "  sindex = 0\n",
        "  eindex = batch_size\n",
        "  while eindex < len(train_data):\n",
        "    batch = train_data[sindex:eindex]\n",
        "    temp = eindex\n",
        "    eindex = eindex + batch_size\n",
        "    sindex = temp\n",
        "    \n",
        "    yield batch"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzNWzwPlV0N6"
      },
      "source": [
        "## 3. Definición del Modelo\n",
        "\n",
        "El modelo a trabajar se basa en una Red Neuronal Recurrente con arquitectura Secuencia a Secuencia o seq2seq. Esta consta de un codificador o encoder que recibe la secuencia de entrada, y de un decodificador o decoder que obtiene la secuencia de salida y la intención correspondiente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "OJru6PCDV0N6"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  # Función de inicialización\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, batch_size=16, n_layers=1):\n",
        "    super(Encoder, self).__init__()\n",
        "    # Se inicializa el tamaño de la secuencia de entrada, el tamaño del vector \n",
        "    # de embedding, el tamaño del vector de estado oculto, el número de capas y \n",
        "    # el tamaño del batch\n",
        "    self.input_size = input_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.n_layers = n_layers\n",
        "    self.batch_size=batch_size\n",
        "    # Se inicializa la capa de embedding y la capa bidireccional LSTM\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, batch_first=True, bidirectional=True)\n",
        "  \n",
        "  # Función para inicializar los pesos\n",
        "  def init_weights(self):\n",
        "    self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "    # self.lstm.weight.data.\n",
        "  \n",
        "  # Función para inicializar con zeros los vectores de estado oculto y de contexto\n",
        "  def init_hidden(self, input):\n",
        "    hidden = Variable(torch.zeros(self.n_layers*2, input.size(0), self.hidden_size)).cuda() if USE_CUDA else Variable(torch.zeros(self.n_layers*2, input.size(0), self.hidden_size))\n",
        "    context = Variable(torch.zeros(self.n_layers*2, input.size(0), self.hidden_size)).cuda() if USE_CUDA else Variable(torch.zeros(self.n_layers*2, input.size(0), self.hidden_size))\n",
        "    return (hidden, context)\n",
        "  \n",
        "  # Función para realizar el cómputo hacia adelante en el codificador\n",
        "  def forward(self, input, input_masking):\n",
        "    \"\"\"\n",
        "    input : Tensor de entrada\n",
        "    input_masking : Tensor de entrada enmascarado para los tokens padding\n",
        "    \"\"\"\n",
        "    # Se inicializan los vectores de estado oculto y de contexto\n",
        "    self.hidden = self.init_hidden(input)\n",
        "    # La secuencia de entrada pasa a través de la capa de embedding y se \n",
        "    # transforma en vector\n",
        "    embedded = self.embedding(input)\n",
        "    # Se calcula la salida y los vectores de estado oculto y de contexto a \n",
        "    # partir del vector de embedding de la secuencia de entrada\n",
        "    output, self.hidden = self.lstm(embedded, self.hidden)\n",
        "    # Vector de salida con el tamaño real de la secuencia (sin padding)\n",
        "    real_context=[]\n",
        "    for i, o in enumerate(output):\n",
        "      real_length = input_masking[i].data.tolist().count(0)\n",
        "      real_context.append(o[real_length-1])\n",
        "            \n",
        "    return output, torch.cat(real_context).view(input.size(0),-1).unsqueeze(1)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPlWktH7V0N7"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  # Función de inicialización\n",
        "  def __init__(self,slot_size,intent_size,embedding_size,hidden_size,batch_size=16,n_layers=1,dropout_p=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "    # Se inicializa el tamaño del vector de estado oculto, el número de slots, \n",
        "    # el número de intenciones, el número de capas, el ratio de dropout, el \n",
        "    # tamaño del vector de embedding y el tamaño del batch\n",
        "    self.hidden_size = hidden_size\n",
        "    self.slot_size = slot_size\n",
        "    self.intent_size = intent_size\n",
        "    self.n_layers = n_layers\n",
        "    self.dropout_p = dropout_p\n",
        "    self.embedding_size = embedding_size\n",
        "    self.batch_size = batch_size\n",
        "    # Se define la capa de embedding\n",
        "    self.embedding = nn.Embedding(self.slot_size, self.embedding_size)\n",
        "\n",
        "    # self.dropout = nn.Dropout(self.dropout_p)\n",
        "\n",
        "    # Se define la capa LSTM del decoder, la capa de Atención y las capas de \n",
        "    # salida de slots e de intención\n",
        "    self.lstm = nn.LSTM(self.embedding_size + self.hidden_size*2, self.hidden_size, self.n_layers, batch_first=True)\n",
        "    self.attn = nn.Linear(self.hidden_size, self.hidden_size) # Attention\n",
        "    self.slot_out = nn.Linear(self.hidden_size*2, self.slot_size)\n",
        "    self.intent_out = nn.Linear(self.hidden_size*2,self.intent_size)\n",
        "  \n",
        "  # Función para inicializar los pesos de la capa de embedding\n",
        "  def init_weights(self):\n",
        "    self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "    # self.out.bias.data.fill_(0)\n",
        "    # self.out.weight.data.uniform_(-0.1, 0.1)\n",
        "    # self.lstm.weight.data.\n",
        "  \n",
        "  # Función para calcular la salida de la capa de Atención\n",
        "  def Attention(self, hidden, encoder_outputs, encoder_maskings):\n",
        "    \"\"\"\n",
        "    hidden : Vector de estado oculto\n",
        "    encoder_outputs : Vector de salida del encoder\n",
        "    encoder_maskings : Vector de salida del encoder enmascarado\n",
        "    \"\"\"\n",
        "    hidden = hidden.squeeze(0).unsqueeze(2)\n",
        "        \n",
        "    batch_size = encoder_outputs.size(0)\n",
        "    max_len = encoder_outputs.size(1)\n",
        "    energies = self.attn(encoder_outputs.contiguous().view(batch_size*max_len,-1))\n",
        "    energies = energies.view(batch_size,max_len,-1)\n",
        "    attn_energies = energies.bmm(hidden).transpose(1,2)\n",
        "    attn_energies = attn_energies.squeeze(1).masked_fill(encoder_maskings,-1e12)\n",
        "    \n",
        "    alpha = F.softmax(attn_energies)\n",
        "    alpha = alpha.unsqueeze(1)\n",
        "    context = alpha.bmm(encoder_outputs)\n",
        "        \n",
        "    return context\n",
        "  \n",
        "  # Función para inicializar con zeros los vectores de estado oculto y de contexto\n",
        "  def init_hidden(self,input):\n",
        "    hidden = Variable(torch.zeros(self.n_layers*1, input.size(0), self.hidden_size)).cuda() if USE_CUDA else Variable(torch.zeros(self.n_layers*2,input.size(0), self.hidden_size))\n",
        "    context = Variable(torch.zeros(self.n_layers*1, input.size(0), self.hidden_size)).cuda() if USE_CUDA else Variable(torch.zeros(self.n_layers*2, input.size(0), self.hidden_size))\n",
        "    return (hidden,context)\n",
        "  \n",
        "  # Función para realizar el cómputo hacia adelante en el decodificador\n",
        "  def forward(self, input, context, encoder_outputs, encoder_maskings, training=True):\n",
        "    \"\"\"\n",
        "    input : Secuencia de entrada para el decoder\n",
        "    enc_context : Vector de contexto transmitido desde el encoder\n",
        "    \"\"\"\n",
        "    # Cálculo de los vectores de embedding para las entradas del decoder\n",
        "    embedded = self.embedding(input)\n",
        "    hidden = self.init_hidden(input)\n",
        "    decode=[]\n",
        "    aligns = encoder_outputs.transpose(0,1)\n",
        "    length = encoder_outputs.size(1)\n",
        "    for i in range(length):\n",
        "      aligned = aligns[i].unsqueeze(1)\n",
        "      _, hidden = self.lstm(torch.cat((embedded,context,aligned),2), hidden)\n",
        "            \n",
        "      # Detección de la intención\n",
        "      if i==0:\n",
        "        intent_hidden = hidden[0].clone()\n",
        "        intent_context = self.Attention(intent_hidden, encoder_outputs,encoder_maskings)\n",
        "        concated = torch.cat((intent_hidden,intent_context.transpose(0,1)),2)\n",
        "        intent_score = self.intent_out(concated.squeeze(0))\n",
        "\n",
        "      concated = torch.cat((hidden[0],context.transpose(0,1)),2)\n",
        "      score = self.slot_out(concated.squeeze(0))\n",
        "      softmaxed = F.log_softmax(score)\n",
        "      decode.append(softmaxed)\n",
        "      _,input = torch.max(softmaxed,1)\n",
        "      embedded = self.embedding(input.unsqueeze(1))\n",
        "      \n",
        "      context = self.Attention(hidden[0], encoder_outputs,encoder_maskings) \n",
        " \n",
        "    slot_scores = torch.cat(decode,1)\n",
        "    return slot_scores.view(input.size(0)*length,-1), intent_score"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQuaG78rV0N8"
      },
      "source": [
        "## 4. Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WQ6y_5IZV0N8"
      },
      "source": [
        "# Parámetros del entrenamiento\n",
        "LEARNING_RATE=0.001\n",
        "EMBEDDING_SIZE=64\n",
        "HIDDEN_SIZE=64\n",
        "BATCH_SIZE=16\n",
        "LENGTH=50\n",
        "STEP_SIZE=10"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK7ygs1aV0N8"
      },
      "source": [
        "# Se instancian las clases del codificador y decodificador\n",
        "encoder = Encoder(len(word2index), EMBEDDING_SIZE, HIDDEN_SIZE)\n",
        "decoder = Decoder(len(tag2index), len(intent2index), len(tag2index) // 3, HIDDEN_SIZE * 2)\n",
        "if USE_CUDA:\n",
        "  encoder = encoder.cuda()\n",
        "  decoder = decoder.cuda()\n",
        "\n",
        "# Se inicializan los pesos del codificador y decodificador\n",
        "encoder.init_weights()\n",
        "decoder.init_weights()\n",
        "\n",
        "# Se definen las funciones de pérdidas y los optimizadores\n",
        "loss_function_1 = nn.CrossEntropyLoss(ignore_index=0)\n",
        "loss_function_2 = nn.CrossEntropyLoss()\n",
        "enc_optim = optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
        "dec_optim = optim.Adam(decoder.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_XVnGg0V0N8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6388d1d8-b4c9-4bb8-b218-5bfb4793e2a5"
      },
      "source": [
        "losses_ = []\n",
        "for step in range(STEP_SIZE):\n",
        "  losses = []\n",
        "  # Se obtienen los batch aleatorios\n",
        "  for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
        "    # Se obtienen los tensores de las secuencias de entrada, salida, máscaras de\n",
        "    # padding (de secuencias de entrada y salida) y las intenciones para los \n",
        "    # ejemplos en el batch\n",
        "    x, y_1, y_2 = zip(*batch)\n",
        "    x = torch.cat(x)\n",
        "    tag_target = torch.cat(y_1)\n",
        "    intent_target = torch.cat(y_2)\n",
        "    x_mask = torch.cat([Variable(torch.ByteTensor(tuple(map(lambda s: s == 0, t.data)))).cuda() if USE_CUDA else Variable(torch.ByteTensor(tuple(map(lambda s: s == 0, t.data)))) for t in x]).view(BATCH_SIZE, -1)\n",
        "    y_1_mask = torch.cat([Variable(torch.ByteTensor(tuple(map(lambda s: s == 0, t.data)))).cuda() if USE_CUDA else Variable(torch.ByteTensor(tuple(map(lambda s: s == 0, t.data)))) for t in tag_target]).view(BATCH_SIZE, -1)\n",
        "    # Se inicializa las gradientes del codificador y decodificador\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    # Se obtienen las salidas del encoder y el tensor de inicio de decodificación\n",
        "    output, hidden_c = encoder(x, x_mask)\n",
        "    start_decode = Variable(torch.LongTensor([[word2index['<SOS>']]*BATCH_SIZE])).cuda().transpose(1,0) if USE_CUDA else Variable(torch.LongTensor([[word2index['<SOS>']]*BATCH_SIZE])).transpose(1,0)\n",
        "    # Se obtienen las salidas del decoder (para los slots y las intenciones)\n",
        "    tag_score, intent_score = decoder(start_decode,hidden_c,output,x_mask)\n",
        "    # Se calcula la función de pérdida en base a las salidas de la red y los \n",
        "    # valores reales\n",
        "    loss_1 = loss_function_1(tag_score, tag_target.view(-1))\n",
        "    loss_2 = loss_function_2(intent_score, intent_target)\n",
        "    # Se suman las pérdidas de ambas salidas (slots e intenciones) y se propaga \n",
        "    # hacia atrás\n",
        "    loss = loss_1 + loss_2\n",
        "    losses.append(loss.data.cpu().numpy() if USE_CUDA else loss.data.numpy())\n",
        "    loss.backward()\n",
        "    # Se utiliza gradrient clipping\n",
        "    torch.nn.utils.clip_grad_norm(encoder.parameters(), 5.0)\n",
        "    torch.nn.utils.clip_grad_norm(decoder.parameters(), 5.0)\n",
        "    # Se optimizan los pesos del paso actual\n",
        "    enc_optim.step()\n",
        "    dec_optim.step()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      print(\"Epoch\", step, \" step\", i, \" : \", np.mean(losses))\n",
        "      losses = []\n",
        "      losses_.append(np.mean(losses))\n",
        "  \n",
        "torch.save(decoder.state_dict(), 'jointnlu-decoder.pkl')\n",
        "torch.save(encoder.state_dict(), 'jointnlu-encoder.pkl')\n",
        "print(\"Entrenamiento completado!\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:937.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0  step 0  :  0.8492794\n",
            "Epoch 0  step 100  :  0.47006452\n",
            "Epoch 0  step 200  :  0.44225731\n",
            "Epoch 1  step 0  :  0.1020063\n",
            "Epoch 1  step 100  :  0.3645543\n",
            "Epoch 1  step 200  :  0.33802676\n",
            "Epoch 2  step 0  :  0.34749222\n",
            "Epoch 2  step 100  :  0.27888924\n",
            "Epoch 2  step 200  :  0.275159\n",
            "Epoch 3  step 0  :  0.47625613\n",
            "Epoch 3  step 100  :  0.22374716\n",
            "Epoch 3  step 200  :  0.21902815\n",
            "Epoch 4  step 0  :  0.13846345\n",
            "Epoch 4  step 100  :  0.15789609\n",
            "Epoch 4  step 200  :  0.1933614\n",
            "Epoch 5  step 0  :  0.14746055\n",
            "Epoch 5  step 100  :  0.1555148\n",
            "Epoch 5  step 200  :  0.15027733\n",
            "Epoch 6  step 0  :  0.091069095\n",
            "Epoch 6  step 100  :  0.123159066\n",
            "Epoch 6  step 200  :  0.13848442\n",
            "Epoch 7  step 0  :  0.068705134\n",
            "Epoch 7  step 100  :  0.10470572\n",
            "Epoch 7  step 200  :  0.095356986\n",
            "Epoch 8  step 0  :  0.08365496\n",
            "Epoch 8  step 100  :  0.07597519\n",
            "Epoch 8  step 200  :  0.07550433\n",
            "Epoch 9  step 0  :  0.12109422\n",
            "Epoch 9  step 100  :  0.069173105\n",
            "Epoch 9  step 200  :  0.06641758\n",
            "Entrenamiento completado!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6DpltobfBnT"
      },
      "source": [
        "# Función para plotear la pérdida\n",
        "def plot_losses(loss):\n",
        "  plt.plot(loss, label='Training loss cada 100 pasos')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "RUcAnz4bfHJq",
        "outputId": "fcea9c3e-f6bb-45ac-89c1-769475868ac1"
      },
      "source": [
        "# Ploteo de la pérdida\n",
        "plot_losses(losess_)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfcUSyFbV0N9"
      },
      "source": [
        "## 5. Prueba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea6JV54SBJIn"
      },
      "source": [
        "Ejecute las tres siguientes celdas si realizará la prueba de forma independiente al entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Yplv745CV0N9"
      },
      "source": [
        "# Si se ejecuta de manera independiente al entrenamiento se debe importar algunos módulos\n",
        "from data import *\n",
        "from model import Encoder, Decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-VgAh-_V0N9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "435fee94-6a62-49ac-d38e-1e4ac4a9ca73"
      },
      "source": [
        "# Función para realizar la carga de los datos y el preprocesamiento descrito anteriormente\n",
        "_, word2index, tag2index, intent2index = preprocessing(\"atis-2.train.w-intent.iob\", 60)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed_data_path : /content/data/\n",
            "Successfully load data. # of set : 4478 \n",
            "# of vocab : 867, # of slot_tag : 120, # of intent_tag : 21\n",
            "Preprocessing complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "RcUdX18iV0N9"
      },
      "source": [
        "# Se invierten algunos diccionarios\n",
        "index2tag = {v:k for k, v in tag2index.items()}\n",
        "index2intent = {v:k for k, v in intent2index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-BSH6itBstG"
      },
      "source": [
        "Ejecute desde acá si lo está trabajando en el mismo notebook o entorno que el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWGhDfKWV0N9"
      },
      "source": [
        "# Se instancian las clases del codificador y decodificador\n",
        "encoder = Encoder(len(word2index), 64, 64)\n",
        "decoder = Decoder(len(tag2index), len(intent2index), len(tag2index) // 3, 64 * 2)\n",
        "\n",
        "# Se cargan los pesos obtenidos en el entrenamiento\n",
        "# Asegurarse de que los pesos se encuentren en la misma ruta para su lectura adecuada\n",
        "encoder.load_state_dict(torch.load('jointnlu-encoder.pkl'))\n",
        "decoder.load_state_dict(torch.load('jointnlu-decoder.pkl'))\n",
        "if USE_CUDA:\n",
        "  encoder = encoder.cuda()\n",
        "  decoder = decoder.cuda()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDEAtCTn2NVz",
        "outputId": "babbb7c6-3f28-4fbf-f3c3-bbad53b9b1a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Obtenemos únicamente el archivo que se utilizará\n",
        "!curl --remote-name -H --location https://raw.githubusercontent.com/yvchen/JointSLU/master/data/atis-2.dev.w-intent.iob"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 92042  100 92042    0     0   665k      0 --:--:-- --:--:-- --:--:--  665k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppKBXPQhV0N-"
      },
      "source": [
        "# Se lee la información del archivo con el corpus de ATIS\n",
        "test = open('atis-2.dev.w-intent.iob', 'r').readlines()\n",
        "test = [t[:-1] for t in test]\n",
        "test = [[t.split('\\t')[0].split(' '), t.split('\\t')[1].split(' ')[:-1], t.split('\\t')[1].split(' ')[-1]] for t in test]\n",
        "test = [[t[0][1:-1],t[1][1:],t[2]] for t in test]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa0ADCUMV0N-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df1cd63d-c4bf-404c-e676-07d65cb10f90"
      },
      "source": [
        "# Se obtienen los tensores de las secuencias de entrada, salida, máscaras de\n",
        "# padding (de secuencias de entrada y salida) y las intenciones para los datos \n",
        "# de prueba\n",
        "index = random.choice(range(len(test)))\n",
        "test_raw = test[index][0]\n",
        "test_in = prepare_sequence(test_raw, word2index)\n",
        "test_mask = Variable(torch.ByteTensor(tuple(map(lambda s: s == 0, test_in.data)))).cuda() if USE_CUDA else Variable(torch.ByteTensor(tuple(map(lambda s: s == 0, test_in.data)))).view(1, -1)\n",
        "start_decode = Variable(torch.LongTensor([[word2index['<SOS>']]*1])).cuda().transpose(1,0) if USE_CUDA else Variable(torch.LongTensor([[word2index['<SOS>']]*1])).transpose(1,0)\n",
        "\n",
        "# Se obtienen las salidas del encoder y con ello las salidas del decoder\n",
        "output, hidden_c = encoder(test_in.unsqueeze(0),test_mask.unsqueeze(0))\n",
        "tag_score, intent_score = decoder(start_decode, hidden_c, output, test_mask)\n",
        "\n",
        "# Se hace un print de las secuencias probadas y sus predicciones\n",
        "v, i = torch.max(tag_score, 1)\n",
        "print(\"Input Sentence : \", *test[index][0])\n",
        "print(\"Truth        : \", *test[index][1])\n",
        "print(\"Prediction : \", *list(map(lambda ii:index2tag[ii], i.data.tolist())))\n",
        "v, i = torch.max(intent_score, 1)\n",
        "print(\"Truth        : \", test[index][2])\n",
        "print(\"Prediction : \", index2intent[i.data.tolist()[0]])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence :  what are the early morning flights from boston to denver\n",
            "Truth        :  O O O B-depart_time.period_of_day B-depart_time.period_of_day O O B-fromloc.city_name O B-toloc.city_name\n",
            "Prediction :  O O O B-depart_time.period_of_day B-depart_time.period_of_day O O B-fromloc.city_name O B-toloc.city_name\n",
            "Truth        :  atis_flight\n",
            "Prediction :  atis_flight\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:937.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    }
  ]
}